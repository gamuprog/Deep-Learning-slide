---
# 表紙
title: "ディープラーニングの技術基礎"
subtitle: "データ変換の「層」を組み合わせて表現学習を実現する"
info: |
  発表時間: 約20分
  対象範囲: テキスト 第3章 セクション1〜4
---

# 本章のテーマ

**ディープラーニングはなぜ高い性能を発揮できるのか？**

その根幹にある **「表現学習」** という考え方と、それを実現する**ニューラルネットワーク**の基本的な仕組みについての解説

1.  **表現学習**: なぜ「表現」が重要なのか？
2.  **ディープラーニングの基礎**: ニューラルネットワークとは何か？
3.  **モデルの構造**: ニューラルネットワークはどのように作られているか？
4.  **モデルの学習**: ニューラルネットワークはどのように学ぶのか？

---
layout: section
---

<div class="text-5xl">

<p>1. 表現学習</p>

</div>

### 「表現」の重要性と難題

---

## 機械学習における最重要課題

機械学習の性能は **「対象の情報をいかに表現するか」** で大きく決まる

- **良い表現**:
  - 後続タスクに必要な情報を保持している
  - 後続タスクが扱いやすい形になっている

- **悪い表現**:
  - 重要な情報が失われ、後続タスクが解けなくなる

<br>
<div class="bg-gray-300 p-4 rounded text-black">
<b>例：</b>一枚の画像を「野球バットを持った少年」という文章で表現すると、「バットをどちらの手で持っているか」という情報は失われる
</div>

---

## 従来の表現方法とその限界

従来、データ表現は専門家が知識に基づいて設計（特徴設計）していた

- **文書**: **BoW (Bag of Words)**
  - 単語の出現頻度などで文書をベクトル表現
  - **課題**: 語順、文脈、単語間の類似性が失われる
  - 例：「私はきつねうどんが好きだ」と「私はうどんが好きなきつねだ」が同じ表現になる

- **画像**: **BoVW (Bag of Visual Words)**
  - 画像内の局所的な特徴（SIFTなど）の集合で表現
  - **課題**: 特徴間の位置関係が失われる
  - 例：「馬の上に人が乗っている」と「人の上に馬が乗っている」が区別できない

---

## ディープラーニングのブレークスルー：表現学習

**表現学習 (Representation Learning)** :データの「表現方法」自体を、データから自動で学習するアプローチ

<br>

<div class="grid grid-cols-2 gap-4">
<div>
<h3 class="text-lg">従来の機械学習</h3>
<div class="p-3 border rounded">

専門家が **特徴表現を設計**

</div>
</div>
<div>
<h3 class="text-lg">ディープラーニング</h3>
<div class="p-3 border rounded">

データから**表現方法を学習**

</div>
</div>
</div>

<br>

**「ディープラーニングは表現学習を実現しているから高性能である」** といっても過言ではない

---
layout: section
---

<div class="text-5xl">

<p>2. ディープラーニングの基礎知識</p>

</div>

### ニューラルネットワークとは何か？

---

## ディープラーニングの正体

ディープラーニング: **層数が多く、幅が広いニューラルネットワーク**を利用した機械学習の手法

<div class="grid grid-cols-2 gap-8 mt-4 text-center">
<div>
<h3 class="font-bold">従来のニューラルネットワーク</h3>
<img src="/assets/1/1.png" class="h-40 mx-auto" alt="Shallow Neural Network">
<ul class="text-sm text-left list-disc pl-6">
<li>層数: ~3層</li>
<li>幅: 10~1000ユニット</li>
</ul>
</div>
<div>
<h3 class="font-bold">ディープラーニング</h3>
<img src="/assets/1/2.png" class="h-40 mx-auto" alt="Deep Neural Network">
<ul class="text-sm text-left list-disc pl-6">
<li>層数: 10~1000層以上</li>
<li>幅: 100~100万ユニット以上</li>
</ul>
</div>
</div>

- **着想**: 脳の神経回路網（ニューロンとシナプス）からヒントを得ています

---

## ニューラルネットワークの本質

ニューラルネットワークは **「合成関数の塊」** のようなもの

- **単純な関数**を大量に組み合わせることで、**複雑な関数**を表現している
- 各関数は**パラメータ**を持っており、これを変えることで挙動を調整することができる
- **万能近似定理**:
  - ニューラルネットワークは、中間層の幅（組み合わせる関数の数）が十分にあれば、**任意の関数を任意の精度で近似できる**ことが証明されている

---
layout: section
---
<div class="text-5xl">

<p>3. ニューラルネットワークのモデル</p>

</div>

### どのように作られているか？

---

## モデルの基本要素：層 (Layer)

ニューラルネットワークは、 **「層 (Layer)」** と呼ばれる計算単位を何層にも重ねて作られる

1つの層は、主に2つの処理から構成される

1.  **線形変換**: 入力に対して重みを掛けて足し合わせる処理- `総結合層`、`畳み込み層`など

2.  **活性化関数**: 線形変換の結果を非線形に変換する処理- `ReLU`、`シグモイド関数`など
<br>
<div class="p-4 bg-gray-300 rounded text-center text-black">
<b>入力</b> &rarr; [ <b>線形変換 (接続層)</b> ] &rarr; [ <b>非線形変換 (活性化関数)</b> ] &rarr; <b>出力</b>
</div>

---

## なぜ「非線形」が重要なのか？

**線形変換だけを何度重ねても、表現力は向上しない**

- 理由: 線形変換の重ね合わせは、結局1つの大きな線形変換で表現できてしまうため
  $$ y = \mathbf{v}^T(\mathbf{W}^T\mathbf{x} + \mathbf{b}) + c \quad \Rightarrow \quad y = \mathbf{u}^T\mathbf{x} + d $$

<br>

線形変換の間に **「非線形の活性化関数」** を挟むことで、初めて層を重ねる意味が生まれる

- これにより、モデルは直線や平面では表現できない、複雑で非線形な関係性を捉えることが可能kanouになる

---

## 多層ネットワークの構造

ディープラーニングでは、この「層」を何段にも積み重ねる ( $f_a$ : 非線形関数)

<div class="text-center my-4">
<p class="text-sm">入力層</p>
<div class="i-carbon:arrow-down text-xl mx-auto"></div>
<div class="border-2 rounded pt-2">
層 1:

$$ \mathbf{h}^{[1]} = f_a(\mathbf{W}^{[1]T}\mathbf{x} + \mathbf{b}^{[1]}) $$

</div>

<div class="i-carbon:arrow-down text-xl mx-auto"></div>
<div class="border-2 rounded pt-2">
層 2:

$$ \mathbf{h}^{[2]} = f_a(\mathbf{W}^{[2]T}\mathbf{h}^{[1]} + \mathbf{b}^{[2]}) $$

</div>
<div class="i-carbon:arrow-down text-xl mx-auto"></div>
<p class="text-sm">...</p>
<div class="i-carbon:arrow-down text-xl mx-auto"></div>
<p class="text-sm">出力層</p>
</div>

---

## ニューラルネットワークの様々な側面

同じモデルでも、異なる視点から捉えることができる

- **関数として**:
  - パラメータ $\theta$ で特徴づけられた、入力 $\mathbf{x}$ から出力 $\mathbf{y}$ への巨大な関数 $y=f(\mathbf{x}; \theta)$

- **神経回路網として**:
  - **ニューロン**（値を持つノード）と、それらを繋ぐ重み付きの**シナプス**（接続）からなるネットワーク脳科学の用語が使われる

- **計算グラフとして**:
  - 計算の流れをノード（演算）とエッジ（データ）で表現した有向グラフ
  - 分岐、合流、パラメータ共有など、複雑な構造を柔軟に設計できる

---
layout: section
---

<div class="text-5xl">

<p>4. ニューラルネットワークの学習</p>

</div>

### どのように学ぶのか？

---

## 学習の目標とプロセス

ニューラルネットワークの学習とは、 **「望ましい出力を得られるように、パラメータを自動で調整すること」**

この学習は、多くの機械学習手法と同様に **「最適化問題」** として定式化される

1.  **目的関数 $L(\theta)$ の設定**:
    - モデルの予測と正解の「間違い度合い」を計算する関数（損失関数）を定義する
    - 例: 訓練データ全体での間違いの平均（訓練誤差）

2.  **最適化**:
    - 目的関数 $L(\theta)$ の値が最小になるようなパラメータ $\theta^*$ を探す

---

## どうやって最適なパラメータを探すか？

ニューラルネットワークのパラメータは数万〜数億個にもなり、目的関数の形は非常に複雑

- **単純な戦略とその問題点**:
  - **1つずつ修正**: パラメータ数がm個の場合、計算量が $O(m^2)$ となり非現実的
  - **ランダムにまとめて修正**: 高次元空間では、改善する方向を偶然見つけられる確率はほぼゼロ

<br>
<div class="p-4 bg-gray-300 rounded text-black">
<b>課題:</b> パラメータ数が多くても、効率的に「改善する方向」を見つける方法が必要
</div>

---

## 勾配降下法による最適化

そこで使われるのが**勾配降下法 (Gradient Descent)**

- **勾配 (Gradient)**:
  - 各パラメータに関する目的関数の**傾き**をまとめたベクトル
  - 目的関数の値が**最も急激に増加する方向**を示す

- **勾配降下法**:
  - 現在のパラメータ位置における勾配を計算する
  - 勾配と**逆の方向**にパラメータを少しだけ更新する
  - このステップを、目的関数の値が十分に小さくなるまで繰り返す

---

# まとめと次のステップ

- **表現学習**:
  - ディープラーニングは、データから最適な**表現方法を学習する**ことで高い性能を実現している

- **ニューラルネットワークのモデル**:
  - **「線形変換」** と **「非線形な活性化関数」** からなる **「層」** を何段にも重ねて作られる

- **ニューラルネットワークの学習**:
  - 学習は**最適化問題**として扱われ、**勾配降下法**を用いてパラメータを更新する

<br>
<hr>
<br>

**次の疑問：**
数億個ものパラメータを持つ複雑な関数の **「勾配」** は、どのようにして効率的に計算されるのか？

&rarr; **次章： 誤差逆伝播法**
