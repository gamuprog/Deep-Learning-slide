---
# 表紙
title: "ディープラーニングを支える技術"
subtitle: "ニューラルネットワーク最大の謎"
info: |
  発表時間: 約10分
  対象範囲: テキスト 第0章
---

# 本章のテーマ

- **2つの謎の提起**
  - なぜ学習でき、なぜ汎化するのか？

- **これから学ぶ生成モデル**
  - データを生成し、世界を理解するモデル。

- **これから学ぶ強化学習**
  - 試行錯誤から学習し、最適な制御を目指す。

- **ディープラーニングと人工知能の課題とこれから**
  - AIが今後どのように発展していくかの展望。

<!-- 第0章なので、本書全体の概要を掴むための章になっている -->

---
layout: section
---

<div class="text-5xl">

<p>1. 2つの大きな謎</p>

</div>

### なぜ学習でき、なぜ汎化するのか？

---

## 謎①：なぜ学習できるのか？ (最適化の謎)

**従来の常識**
ニューラルネットワークの学習は、最適化が難しいと考えられていた。

- **問題点**: 学習の目的関数は**非凸関数**であり、最適化を妨げる**極小解**、**プラトー**、**鞍点**が無数に存在する。

<br>

**しかし、実際には…**

- 大規模なネットワークでも、勾配降下法で安定して学習が成功する。
- どのような初期値から始めても、ほぼ同じ性能の解に到達できる。

<br>

<div class="p-3 bg-gray-300 rounded text-black text-center">

<b>なぜこの難しい最適化問題が解けてしまうのか？ &rarr; 第1章のテーマ</b>

</div>

---

## 謎②：なぜ汎化するのか？ (汎化の謎)

<br>

**従来の常識**
モデルのパラメータ数がデータ数より多いと、訓練データを丸暗記してしまい、未知のデータに対応できなくなる (過学習)。

- **問題点**: ニューラルネットワークは、パラメータ数がデータ数を遥かに上回る**過剰パラメータ表現**である。

<br>

**しかし、実際には…**

- パラメータ数が多くても過学習しにくく、高い汎化性能を達成する。
- さらに、モデルサイズが大きいほど学習効率や汎化性能が向上することさえある。

<br>

<div class="p-3 bg-gray-300 rounded text-black text-center">

<b>なぜ過剰なパラメータを持つのに汎化できるのか？ &rarr; 第2章のテーマ</b>

</div>

---
layout: section
---

<div class="text-5xl">

<p>2. ディープラーニングの応用</p>

</div>

### 生成モデルと強化学習

---

## 応用①：深層生成モデル

分類や予測といったタスクに加え、「生成」を扱う**生成モデル**もディープラーニングの得意分野である。

**生成モデルのできること**

<div class="grid grid-cols-3">
<div class="col-span-2 text-sm">

- **データの生成**
  - 画像、音声、テキストなど、高品質のデータを生成できる。
- **尤度の評価**
  - 与えられたデータがどれだけ「もっともらしいか」を確率として評価できる。
- **条件付き生成**
  - 「テキストから画像」のように、条件に応じた多様なデータを生成できる。
- **表現学習**
  - データの生成過程を学ぶことで、データの「もつれを解いた表現」を獲得できる。

</div>
<div>

<img src="/assets/2/0.png" class="h-65 mx-auto" alt="Grid Search">

</div>
</div>
<br>

<div class="p-3 bg-gray-300 rounded text-black text-center">

<b>生成モデルの原理と応用 &rarr; 第3章のテーマ</b>

</div>

---

## 応用②：深層強化学習

ディープラーニングと**強化学習**の融合は、AlphaGoに代表されるように、AIの能力を飛躍的に向上させた。

<div class="grid grid-cols-5">
<div class="col-span-3">

**強化学習とは**

- エージェントが環境との相互作用の中で**試行錯誤**を繰り返す。
- 「報酬」を最大化するように行動を学習していく手法である。

</div>
<div class="col-span-2">

<img src="/assets/2/2.png" class="h-45 mx-auto" alt="Grid Search">
</div>
</div>

**ディープラーニングの役割**

- 強化学習においても、タスクを解く上で **「表現学習」** が重要である。
- 状態（例：ゲーム画面）から価値や方策を学習するための表現を、データから獲得する役割を担う。

<br>

<div class="p-3 bg-gray-300 rounded text-black text-center">

<b>強化学習の仕組みとディープラーニングとの融合 &rarr; 第4章のテーマ</b>

</div>

---
layout: section
---

<div class="text-5xl">

<p>3. 今後の課題と展望</p>

</div>

### ディープラーニングとAIのこれから

---

## これからのディープラーニングとAI

人の知能と比較すると、ディープラーニングにはまだ多くの課題が残っている。

本書の最終章では、今後の発展に向けた4つの重要テーマを議論する。

1.  **学習手法の発展**
    - 大量の教師ありデータへの依存を減らす**自己教師あり学習**。

2.  **計算性能との関係**
    - AIの発展を支えてきた計算性能の向上のトレンドと、今後の付き合い方。

3.  **問題固有の知識の導入**
    - 現実世界が持つ**幾何学**や**対称性**（不変性・同変性）をモデルに組み込む。

4.  **分布外汎化と抽象的思考**
    - 人間の思考における **「システム2」** （遅い思考）を実現し、未知の状況に対応する能力。

---

# まとめ

- **本書の構成**: ディープラーニングの **「最適化」** と **「汎化」** という2大ミステリーの解明から始まる。

- **応用展開**: さらに **「生成モデル」** と **「強化学習」** という、AIの能力を大きく広げる2つの応用分野を解説する。

- **将来展望**: 最後に、今後のAIの発展に不可欠な **「学習手法」「計算性能」「対称性」「システム2」** といったテーマを考察する。

<br>

それでは、最初の謎である **「なぜ学習できるのか」** から話を始める。

---

## ハイパーパラメータとは？

モデルの学習を始める前に、人間が予め決めておく必要があるパラメータのことである。

- 誤差逆伝播法と勾配降下法では**自動で決定されない**。
- 学習効率や最終的なモデルの**汎化性能**に大きな影響を及ぼす。
- いわば「モデルパラメータを決めるためのパラメータ」である。

---

## ハイパーパラメータの3つの種類

ハイパーパラメータは、大きく3種類に分類される。

1.  **最適化の挙動を決める**
    - **学習率**、モーメンタムの係数($\beta$)、ミニバッチサイズなど。

2.  **正則化の強さや挙動を決める**
    - **Weight Decay** の係数、**ドロップアウト**率など。
    - これらはモデルの過学習を防ぎ、汎化性能を高めるために重要である。

3.  **ネットワークアーキテクチャを決める**
    - 層の数（深さ）、各層のユニット数（幅）、活性化関数の種類など。

---
layout: section
---

<div class="text-5xl">

<p>ハイパーパラメータの調整方法</p>

</div>

---

## どのように調整するのか？

勾配降下法によるモデルパラメータの学習とは**別のプロセス**で調整を行う必要がある。

**基本的なプロセス**

1.  調整したいハイパーパラメータの候補（範囲や値）を決める。
2.  各候補の組み合わせでモデルを**訓練データ**で学習させる。
3.  学習済みモデルを**検証用データ**で評価する。
4.  検証用データで最も性能が良かったハイパーパラメータの組み合わせを採用する。

---

## 探索手法：グリッド探索 vs ランダム探索

候補の組み合わせをどのように選ぶかには、代表的な手法が2つある。

<div class="grid grid-cols-2 gap-8 mt-4 text-center">

<div>

<h3 class="font-bold">グリッド探索 (Grid Search)</h3>

<img src="/assets/2/grid.png" class="h-40 mx-auto" alt="Grid Search">

<p class="text-sm">各パラメータの候補値を格子状に区切り、全ての組み合わせを試す手法。</p>

</div>

<div>

<h3 class="font-bold">ランダム探索 (Random Search)</h3>

<img src="/assets/2/random.png" class="h-40 mx-auto" alt="Random Search">

<p class="text-sm">各パラメータの値を設定した範囲からランダムにサンプリングして試す手法。</p>

</div>

</div>

<div class="mt-4 p-3 bg-gray-300 rounded text-black">

性能に影響する重要なパラメータが**一部**の場合、**ランダム探索**の方が多くのパラメータを評価できるため優れている。

</div>

---
layout: section
---

<div class="text-5xl">

<p>特に重要なハイパーパラメータ</p>

</div>

---

## 最も重要なハイパーパラメータ：学習率

ハイパーパラメータの中でも、**学習率**は最も重要である。

- **収束速度**だけでなく、モデルの**汎化性能**にも大きく影響する。
- **小さすぎる場合**: 収束が遅く、性能の低い解に陥りやすい。
- **大きすぎる場合**: 学習が発散してしまい、収束しない。

<br>

**基本戦略**

**発散しない範囲でできるだけ大きな学習率から始め、徐々に下げていく** &rarr; **アニーリング** (焼きなまし法)

---

## 学習率スケジューリング戦略

学習率を動的に変化させることで、学習の安定化と性能向上が期待できる。

<div class="grid grid-cols-2 gap-8 mt-4 text-center">

<div>

<h3 class="font-bold">ウォームアップ (Warmup)</h3>

<p class="text-sm">学習の初期段階では学習率を小さく設定し、徐々に目標値まで上げていく手法。学習初期の不安定な勾配による発散を防ぐ効果がある。</p>

</div>

<div>

<h3 class="font-bold">アニーリング (Annealing)</h3>

<p class="text-sm">ウォームアップ後、学習が進むにつれて学習率を徐々に下げていく手法。解への収束を助ける。</p>

</div>

</div>

<img src="/assets/2/1.png" class="h-50 mx-auto mt-4" alt="Learning Rate Schedule">

<p class="text-sm text-center">代表的な手法として、コサインカーブに従って学習率を減衰させる<b>コサインアニーリング</b>がある。</p>

---

## ミニバッチサイズ

学習率と密接な関係にあるのが**ミニバッチサイズ**である。

- **役割**: 確率的勾配降下法における、勾配のノイズの大きさを決める。
- **汎化性能への影響**: バッチサイズを変えると、暗黙的に加わるノイズの大きさが変わり、汎化性能に影響を与える。

<br>

<div class="p-3 bg-gray-300 rounded text-black">

あるバッチサイズ、学習率で汎化性能が最大化されている時、バッチサイズをa倍にする場合は、学習率もa倍にすると同程度の汎化性能を達成できる。

</div>

---

# まとめ

- **ハイパーパラメータとは**:
  - 学習前に人間が設定するパラメータで、学習の挙動と最終性能を大きく左右する。
  - **最適化**・**正則化**・**アーキテクチャ**の3種類に大別される。

- **調整方法**:
  - 検証用データを用いて性能を評価し、グリッド探索やランダム探索などの手法で最適な組み合わせを探す。

- **特に重要なもの**:
  - **学習率**が最も重要であり、**ウォームアップ**や**アニーリング**といったスケジュール戦略が学習の安定化と性能向上に有効である。
  - **ミニバッチサイズ**も学習率と関連し、汎化性能に影響を与える。
