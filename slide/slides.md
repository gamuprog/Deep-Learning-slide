---
# 表紙
title: "VAE (変分オートエンコーダ)"
subtitle: "ニューラルネットワークを使った潜在変数モデル"
info: |
  発表時間: 約20分
  対象範囲: テキスト 第3章 3-2
---

# はじめに：VAEとは？

  - **深層生成モデルの代表例**

      - GAN (敵対的生成モデル)と並んで、最も広く使われている生成モデルの一つ。

  - **潜在変数モデル**

      - 観測できない「潜在変数」からデータを生成するモデル。
      - これにより、データ全体を要約した抽象的な表現を獲得できる。

  - **認識と生成の同時学習**

      - データ生成（復号化器）だけでなく、データから潜在変数を推定する認識（符号化器）も同時に学習する。

---
layout: section
---

<div class="text-5xl">

<p>VAEの仕組み</p>

</div>

### どのようにデータを生成し、学習するのか？

---

## 基本構造：符号化器 (Encoder) と 復号化器 (Decoder)

VAEは、役割の異なる2つのニューラルネットワークで構成される。

<div class="grid grid-cols-2 gap-8 mt-4 text-center">
<div>

<h3 class="font-bold">符号化器 (Encoder)</h3>
<p class="text-sm">データ<b>x</b>を入力し、それが潜在空間のどのあたりに位置するか (確率分布) を出力する。「認識モデル」とも呼ばれる。</p>
<br>
<b>データ &rarr; 潜在空間の分布</b>

</div>
<div>

<h3 class="font-bold">復号化器 (Decoder)</h3>
<p class="text-sm">潜在空間からサンプリングした潜在変数<b>z</b>を入力し、新しいデータ<b>x'</b>を生成する。「生成モデル」とも呼ばれる。</p>
<br>
<b>潜在変数 &rarr; データ</b>

</div>
</div>

<div class="mt-8 text-center">
<p>データ生成時には<b>復号化器のみ</b>を使用する。</p>
</div>

-----

## データ生成のプロセス

VAEは、以下のステップで新しいデータを生成する。

<div class="text-center my-4">
<p class="text-lg">❶ 単純な確率分布 (正規分布) から<b>潜在変数 z</b> をランダムにサンプリング</p>
<p class="text-3xl">&darr;</p>
<p class="text-lg">❷ <b>復号化器</b>が z を受け取り、生成したいデータの分布のパラメータ (平均μ, 分散σ) を出力</p>
<p class="text-3xl">&darr;</p>
<p class="text-lg">❸ その分布から<b>新しいデータ x'</b> をサンプリング (生成)</p>
</div>

<div class="p-3 bg-gray-300 rounded text-black text-center">
無数の単純な正規分布を組み合わせることで、画像のような複雑なデータ分布を表現できる。
</div>

-----

## 学習の全体像

学習時は、**符号化器**と**復号化器**の両方を使って、入力と出力が似るように学習する。

<div class="text-center my-2">
<p class="text-lg">入力データ <b>x</b></p>
<p class="text-2xl">&darr;</p>
<p class="text-lg"><b>符号化器 (Encoder)</b> で潜在空間の分布 (平均μ, 分散σ) に変換</p>
<p class="text-2xl">&darr;</p>
<p class="text-lg">その分布から<b>潜在変数 z</b> をサンプリング (+ノイズ)</p>
<p class="text-2xl">&darr;</p>
<p class="text-lg"><b>復号化器 (Decoder)</b> で z からデータを復元</p>
<p class="text-2xl">&darr;</p>
<p class="text-lg">復元データ <b>x'</b></p>
</div>
<div class="p-2 text-sm bg-gray-300 rounded text-black text-center">
最終的に、入力<b>x</b>と復元<b>x'</b>の差 (誤差) が小さくなるように、両方のネットワークのパラメータを更新していく。
</div>

---
layout: section
---

<div class="text-5xl">

<p>オートエンコーダとの違いと学習の工夫</p>

</div>

-----

## VAE vs オートエンコーダ

学習のプロセスは、データを圧縮・復元する **オートエンコーダ (AE)** に非常によく似ている。

<div class="grid grid-cols-2 gap-8 mt-4">
<div>

<h3 class="font-bold">オートエンコーダ (AE)</h3>

  - 潜在変数を、潜在空間上の**一点**として扱う。
  - 入力xに対して、潜在変数zは一意に決まる。
  - 主な目的は次元削減や特徴抽出であり、新しいデータを生成する能力は低い。

</div>
<div>

<h3 class="font-bold">変分オートエンコーダ (VAE)</h3>

  - 潜在変数を、潜在空間上の**確率分布 (領域)** として扱う。
  - 「このあたりにありそうだ」という曖昧な領域 (平均μ, 分散σ) を学習する。
  - この「曖昧さ (確率分布)」があるため、潜在空間からサンプリングすることで**未知の新しいデータを生成できる**。

</div>
</div>

<div class="mt-4 p-3 bg-gray-300 rounded text-black text-center">

決定的な違いは、潜在空間を **確率的** に扱うかどうか。これによりVAEは強力な **生成モデル** として機能する。

</div>

-----

## 学習の難しさと工夫：変数変換トリック

**課題**: 符号化器の学習時、確率的なサンプリング処理が誤差逆伝播の計算を妨げてしまう。

**解決策**: **変数変換トリック (Reparameterization Trick)** を使う。

  - **アイデア**: 確率的なサンプリングを、学習パラメータとは無関係な「外部のノイズ」として扱うように数式を変換する。
  - **変更前**: $z 〜 {N}( μ, σ )$ （zはμとσに依存する確率変数）
  - **変更後**: $z = μ + σ*ε$ (ただし $ε 〜 {N}(0, 1)$)

<div class="p-3 bg-gray-300 rounded text-black text-center">
このトリックにより、ネットワーク全体で誤差逆伝播が可能になり、自己符号化器のようにシンプルに学習できるようになった。
</div>

---
layout: section
---

<div class="text-5xl">

<p>まとめ</p>

</div>

-----

## まとめ：VAEのポイント

  - **生成モデル**: ニューラルネットワークを使った**潜在変数モデル**であり、新しいデータを生成できる。

  - **構造**: **符号化器** (認識) と**復号化器** (生成) から成り、学習時はオートエンコーダのように振る舞う。

  - **AEとの違い**: 潜在空間を**確率分布**として扱うことで、生成能力を獲得している。

  - **学習の工夫**: **変数変換トリック**により、誤差逆伝播法を用いた効率的な学習を可能にしている。

  - **特徴**: 学習が比較的安定しており、データの本質的な特徴を捉えた**抽象的な表現**を獲得できる。